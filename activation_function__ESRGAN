# 3.6. Activation Function: Leaky ReLU:


import torch
import torch.nn as nn

# Define a simple neural network layer with Leaky ReLU activation
class LeakyReLULayer(nn.Module):
    def __init__(self, negative_slope=0.2):
        super(LeakyReLULayer, self).__init__()
        self.negative_slope = negative_slope
        self.activation = nn.LeakyReLU(negative_slope=self.negative_slope)

    def forward(self, x):
        return self.activation(x)

# Example usage
input_tensor = torch.randn(1, 64, 32, 32)  # Random input tensor
leaky_relu_layer = LeakyReLULayer(negative_slope=0.2)
output_tensor = leaky_relu_layer(input_tensor)
